{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROCT-Net_Kermany Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IXdIzj_DuoW_gkuEbVv6h5hUyYVZoFjB",
      "authorship_tag": "ABX9TyNbxmCAcKW51/Elh2gPqp3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr7495/OCT-classification/blob/main/ROCT_Net_Kermany_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IldnlMR_t1BM"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGVG8ZcmVIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d0953a-4c6e-4879-92ee-b9cb58416c56"
      },
      "source": [
        "!pip install git+https://github.com/keras-team/keras-contrib"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/keras-team/keras-contrib\n",
            "  Cloning https://github.com/keras-team/keras-contrib to /tmp/pip-req-build-a_jrv43j\n",
            "  Running command git clone -q https://github.com/keras-team/keras-contrib /tmp/pip-req-build-a_jrv43j\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.8.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=f6e4bad45f96fca69da42704b077432bf468f63616ad8f847e2e94b3e25b4d42\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3x7q2903/wheels/8e/09/42/ae2d52e8651acfb0595f0f271e668a85ace2f9eb92022307ab\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzQOkYDRaizC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfed355-64af-490e-80b1-e8d9a28dd731"
      },
      "source": [
        "pip install tensorflow_addons"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 194 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 204 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 225 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 235 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 256 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 276 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 286 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 307 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 317 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 337 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 358 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 368 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 378 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 389 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 399 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 409 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 430 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 440 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 450 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 460 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 471 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 481 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 491 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 501 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 512 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 522 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 532 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 542 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 552 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 563 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 573 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 583 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 604 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 614 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 624 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 634 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 645 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 655 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 665 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 675 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 686 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 696 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 706 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 716 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 727 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 737 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 747 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 757 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 768 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 778 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 788 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 798 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 808 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 819 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 829 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 839 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 849 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 860 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 870 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 880 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 890 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 901 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 911 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 921 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 931 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 942 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 952 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 962 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 972 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 983 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 993 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgrUHaGTuM-8"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras_contrib.layers import Capsule\n",
        "from keras_contrib.activations import squash"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqjUPmNDjH1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b41468f6-2d38-4cdd-dd27-335b52c079d3"
      },
      "source": [
        "!wget -cO - 'https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/rscbjbr9sj-3.zip' > data.zip # Download dataset"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-03 23:16:28--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/rscbjbr9sj-3.zip\n",
            "Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 52.218.118.58\n",
            "Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|52.218.118.58|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8432836958 (7.9G) [application/octet-stream]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   7.85G  28.6MB/s    in 4m 44s  \n",
            "\n",
            "2022-03-03 23:21:12 (28.3 MB/s) - written to stdout [8432836958/8432836958]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTl5cmF-vFmc"
      },
      "source": [
        "archive = zipfile.ZipFile('data.zip') \n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, '.')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOkqWUduv6oT"
      },
      "source": [
        "archive = zipfile.ZipFile('ZhangLabData.zip') \n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, 'data')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZINuk8k2wC9E"
      },
      "source": [
        "def find(ch,st):\n",
        "    indexes=[]\n",
        "    index1=0\n",
        "    while(True):\n",
        "        if ch in st[index1:]:\n",
        "            index2=st[index1:].index(ch)\n",
        "            indexes.append(index1+index2)\n",
        "            index1=index2+index1+1\n",
        "        \n",
        "        else:\n",
        "            break\n",
        "    return(indexes)\n",
        "\n",
        "#Here we create Ground-Truth CSV files\n",
        "\n",
        "train={'CNV':{},'DME':{},'DRUSEN':{},'NORMAL':{}}\n",
        "test={'CNV':{},'DME':{},'DRUSEN':{},'NORMAL':{}}\n",
        "\n",
        "for r,d,f in os.walk('data/CellData/OCT/train'):\n",
        "    for file in f:\n",
        "        inds=find('-',file)\n",
        "        pid=file[inds[0]+1:inds[1]]\n",
        "        if 'CNV' in r:\n",
        "            if pid not in train['CNV']:\n",
        "                train['CNV'][pid]=[]\n",
        "            train['CNV'][pid].append('{}/{}'.format('CNV',file))\n",
        "        elif 'DME' in r:\n",
        "            if pid not in train['DME']:\n",
        "                train['DME'][pid]=[]\n",
        "            train['DME'][pid].append('{}/{}'.format('DME',file))\n",
        "        elif 'DRUSEN' in r:\n",
        "            if pid not in train['DRUSEN']:\n",
        "                train['DRUSEN'][pid]=[]\n",
        "            train['DRUSEN'][pid].append('{}/{}'.format('DRUSEN',file))\n",
        "        elif 'NORMAL' in r:\n",
        "            if pid not in train['NORMAL']:\n",
        "                train['NORMAL'][pid]=[]\n",
        "            train['NORMAL'][pid].append('{}/{}'.format('NORMAL',file))\n",
        "            \n",
        "for r,d,f in os.walk('data/CellData/OCT/test'):\n",
        "    for file in f:\n",
        "        inds=find('-',file)\n",
        "        pid=file[inds[0]+1:inds[1]]\n",
        "        if 'CNV' in r:\n",
        "            if pid not in test['CNV']:\n",
        "                test['CNV'][pid]=[]\n",
        "            test['CNV'][pid].append('{}/{}'.format('CNV',file))\n",
        "        elif 'DME' in r:\n",
        "            if pid not in test['DME']:\n",
        "                test['DME'][pid]=[]\n",
        "            test['DME'][pid].append('{}/{}'.format('DME',file))\n",
        "        elif 'DRUSEN' in r:\n",
        "            if pid not in test['DRUSEN']:\n",
        "                test['DRUSEN'][pid]=[]\n",
        "            test['DRUSEN'][pid].append('{}/{}'.format('DRUSEN',file))\n",
        "        elif 'NORMAL' in r:\n",
        "            if pid not in test['NORMAL']:\n",
        "                test['NORMAL'][pid]=[]\n",
        "            test['NORMAL'][pid].append('{}/{}'.format('NORMAL',file)) \n",
        "    \n",
        "        \n",
        "with open('train_cell.csv','w',newline='') as f:\n",
        "  csvw=csv.writer(f)\n",
        "  csvw.writerow(['filename','class'])\n",
        "  for disease in train:\n",
        "      if disease=='NORMAL':\n",
        "          count=0\n",
        "          for pid in train[disease]:\n",
        "              csvw.writerow([train[disease][pid][0],disease])\n",
        "              count+=1\n",
        "              if count==1000:\n",
        "                  break\n",
        "      else:\n",
        "          for pid in train[disease]:\n",
        "              csvw.writerow([train[disease][pid][0],disease])\n",
        "    \n",
        "\n",
        "with open('test_cell.csv','w',newline='') as f:\n",
        "  csvw=csv.writer(f)\n",
        "  csvw.writerow(['filename','class'])\n",
        "  for disease in test:\n",
        "      for pid in test[disease]:\n",
        "          for item in test[disease][pid]:\n",
        "              csvw.writerow([item,disease])\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHY3ZdTDU1ar"
      },
      "source": [
        "\n",
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,vertical_flip=True\n",
        "                                                             ,zoom_range=0.1,rotation_range=360\n",
        "                                                             ,width_shift_range=0.1,height_shift_range=0.1)\n",
        "\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "train_df = pd.read_csv(\"train_cell.csv\")\n",
        "test_df = pd.read_csv(\"test_cell.csv\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgctU8dyU1hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe7c75e-53df-4bfc-aeca-1a427aa1e1ad"
      },
      "source": [
        "shape=(512, 512)\n",
        "batch_size=8 #increase when having better GPU\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "      dataframe=train_df,\n",
        "      directory='data/CellData/OCT/train',\n",
        "      x_col=\"filename\",\n",
        "      y_col=\"class\",\n",
        "      target_size=shape,\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical',shuffle=True)\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        directory='data/CellData/OCT/test',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=shape,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=True)\n",
        "train_img_num=len(train_generator.filenames)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3213 validated image filenames belonging to 4 classes.\n",
            "Found 1000 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDZknqB4ZKuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515733cb-0f4f-48c8-abd9-01919d34ea77"
      },
      "source": [
        "!git clone https://github.com/mhrahimzadeh1374/automl"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'automl'...\n",
            "remote: Enumerating objects: 3922, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 3922 (delta 193), reused 215 (delta 165), pack-reused 3613\u001b[K\n",
            "Receiving objects: 100% (3922/3922), 23.18 MiB | 22.97 MiB/s, done.\n",
            "Resolving deltas: 100% (2945/2945), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNiNGUQoZ9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe6c63f3-f5e1-4d92-f6a3-61b1cd6ae0b3"
      },
      "source": [
        "cd automl/efficientnetv2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/automl/efficientnetv2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVA1nBE4PsfD"
      },
      "source": [
        "from effnetv2_model import get_model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8L3JCAfhY-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87eba38-639c-41e1-ceb6-f4654f42c002"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/automl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIWPFZ-rhZ-Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec13c60-f301-4096-dbf6-a7ae751b8f2a"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js83s0PpU1vL"
      },
      "source": [
        "name=\"ROCT-Net_Kermany Dataset\"\n",
        "!mkdir \"models\"\n",
        "keras.backend.clear_session()\n",
        "input_tensor=keras.Input(shape=(shape[0],shape[1],3))\n",
        "base_model1=get_model('efficientnetv2-b0', include_top=False, pretrained=True)(input_tensor) #load EfficientNetV2-B0\n",
        "base_model2=keras.applications.Xception(input_tensor=input_tensor,weights='imagenet',include_top=False)(input_tensor) #load Xception\n",
        "\n",
        "concatenated=keras.layers.concatenate([base_model1,base_model2])  #load concatenated model\n",
        "\n",
        "avg=keras.layers.AveragePooling2D(3,padding='valid')(concatenated) #deploy Wise-srNet\n",
        "depthw=keras.layers.DepthwiseConv2D(5,\n",
        "                                      depthwise_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                                      bias_initializer=keras.initializers.Zeros(),depthwise_constraint=keras.constraints.NonNeg())(avg)\n",
        "# Define Capsules\n",
        "capsule = Capsule(num_capsule=10,\n",
        "                dim_capsule=16,\n",
        "                routings=3,\n",
        "                activation=squash,\n",
        "                share_weights=True)(depthw)\n",
        "\n",
        "flat=keras.layers.Flatten()(capsule)\n",
        "dp=keras.layers.Dropout(0.2)(flat)\n",
        "preds=keras.layers.Dense(4,activation='softmax',\n",
        "                          kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                          bias_initializer=keras.initializers.Zeros(),)(dp)\n",
        "model=keras.Model(inputs=input_tensor, outputs=preds)  \n",
        "\n",
        "##################################\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "model.summary()\n",
        "filepath=\"models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5\"%name\n",
        "\n",
        "#Note that keras classic saving function would not save capsule weights, so the saved model will not be as equal as the trained model. This bug must be fixed in the future.\n",
        "\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max',save_weights_only=True) #creating checkpoint to save the best validation accuracy\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "lr_schedule =keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.045,\n",
        "    decay_steps=2*int(train_img_num/batch_size),\n",
        "    decay_rate=0.94,\n",
        "    staircase=True)\n",
        "optimizer=keras.optimizers.SGD(momentum=0.9,learning_rate=lr_schedule)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "hist=model.fit_generator(train_generator, epochs=40,validation_data=validation_generator,shuffle=True,callbacks=callbacks_list) #start training\n",
        "with open('{}-results.csv'.format(name), mode='w',newline='') as csv_file: #write evaluation metrics\n",
        "  csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  for key in hist.history:\n",
        "    data=[key]\n",
        "    data.extend(hist.history[key])\n",
        "    csv_writer.writerow(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ROCT-Net_OCTID dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1yEkZuo9cztDU2D7VneXV7tcgRcInKcpu",
      "authorship_tag": "ABX9TyOjO/suLSNVmhy/Gn18rmJQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mr7495/OCT-classification/blob/main/ROCT_Net_OCTID_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IldnlMR_t1BM"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGVG8ZcmVIL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "703872c7-ab80-48db-c83d-7db525749605"
      },
      "source": [
        "!pip install git+https://github.com/keras-team/keras-contrib"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/keras-team/keras-contrib\n",
            "  Cloning https://github.com/keras-team/keras-contrib to /tmp/pip-req-build-l0d7czvs\n",
            "  Running command git clone -q https://github.com/keras-team/keras-contrib /tmp/pip-req-build-l0d7czvs\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.8.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=ab26a08a4cd74d1525180459a101449ee79854e52c579d7220a44eebfdd60de8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6i96xt16/wheels/8e/09/42/ae2d52e8651acfb0595f0f271e668a85ace2f9eb92022307ab\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzQOkYDRaizC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3835631-51b4-4696-9830-82cceb778bc9"
      },
      "source": [
        "pip install tensorflow_addons"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgrUHaGTuM-8"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import cv2\n",
        "import zipfile\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from keras_contrib.layers import Capsule\n",
        "from keras_contrib.activations import squash"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2Q9Atq7k7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c3a518-8929-4f93-a2e7-deff81ffab73"
      },
      "source": [
        "!gdown --id \"1okQhlbA6RNJ23fQG_7z-zcZEiCfCT8pb\" #if this code failed, get the dataset from https://drive.google.com/file/d/1okQhlbA6RNJ23fQG_7z-zcZEiCfCT8pb/view?usp=sharing"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1okQhlbA6RNJ23fQG_7z-zcZEiCfCT8pb \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTl5cmF-vFmc"
      },
      "source": [
        "archive = zipfile.ZipFile('OCTA-gholami.zip') \n",
        "for file in archive.namelist():\n",
        "     archive.extract(file, '.')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHY3ZdTDU1ar"
      },
      "source": [
        "train_datagen = keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,vertical_flip=True\n",
        "                                                             ,zoom_range=0.1,rotation_range=360\n",
        "                                                             ,width_shift_range=0.1,height_shift_range=0.1)\n",
        "\n",
        "test_datagen = keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "train_df = pd.read_csv(\"train_gholami.csv\")\n",
        "test_df = pd.read_csv(\"test_gholami.csv\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgctU8dyU1hq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66587e4-22d3-46d8-c6b4-f6c95c6a981e"
      },
      "source": [
        "#create dataloader\n",
        "shape=(512, 512)\n",
        "batch_size=10\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "      dataframe=train_df,\n",
        "      directory='gholami',\n",
        "      x_col=\"filename\",\n",
        "      y_col=\"class\",\n",
        "      target_size=shape,\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical',shuffle=True)\n",
        "validation_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=test_df,\n",
        "        directory='gholami',\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"class\",\n",
        "        target_size=shape,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',shuffle=True)\n",
        "train_img_num=len(train_generator.filenames)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 459 validated image filenames belonging to 5 classes.\n",
            "Found 113 validated image filenames belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDZknqB4ZKuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2efda911-913c-4f87-8763-387dfb8f7320"
      },
      "source": [
        "!git clone https://github.com/mhrahimzadeh1374/automl"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'automl'...\n",
            "remote: Enumerating objects: 3922, done.\u001b[K\n",
            "remote: Counting objects: 100% (309/309), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 3922 (delta 193), reused 217 (delta 164), pack-reused 3613\u001b[K\n",
            "Receiving objects: 100% (3922/3922), 23.18 MiB | 12.78 MiB/s, done.\n",
            "Resolving deltas: 100% (2945/2945), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNiNGUQoZ9ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e30fe04-47e4-428c-ed7b-e65c802f6a11"
      },
      "source": [
        "cd automl/efficientnetv2 "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/automl/efficientnetv2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVA1nBE4PsfD"
      },
      "source": [
        "from effnetv2_model import get_model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8L3JCAfhY-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d74fa3ac-03ed-430e-f4c5-529678c8912f"
      },
      "source": [
        "cd ../../"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js83s0PpU1vL"
      },
      "source": [
        "name=\"ROCT-Net_OCTID dataset\"\n",
        "keras.backend.clear_session()\n",
        "input_tensor=keras.Input(shape=(shape[0],shape[1],3))\n",
        "base_model1=get_model('efficientnetv2-b0', include_top=False, pretrained=True)(input_tensor) #load EfficientNetV2-B0\n",
        "base_model2=keras.applications.Xception(input_tensor=input_tensor,weights='imagenet',include_top=False)(input_tensor) #load Xception\n",
        "\n",
        "concatenated=keras.layers.concatenate([base_model1,base_model2]) #load concatenated model\n",
        "\n",
        "avg=keras.layers.AveragePooling2D(3,padding='valid')(concatenated) #deploy Wise-srNet\n",
        "depthw=keras.layers.DepthwiseConv2D(5,\n",
        "                                      depthwise_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                                      bias_initializer=keras.initializers.Zeros(),depthwise_constraint=keras.constraints.NonNeg())(avg)\n",
        "# Define Capsules\n",
        "capsule = Capsule(num_capsule=10,\n",
        "                dim_capsule=16,\n",
        "                routings=3,\n",
        "                activation=squash,\n",
        "                share_weights=True)(depthw)\n",
        "\n",
        "flat=keras.layers.Flatten()(capsule)\n",
        "dp=keras.layers.Dropout(0.2)(flat)\n",
        "preds=keras.layers.Dense(5,activation='softmax',\n",
        "                          kernel_initializer=keras.initializers.RandomNormal(mean=0.0,stddev=0.01),\n",
        "                          bias_initializer=keras.initializers.Zeros(),)(dp)\n",
        "model=keras.Model(inputs=input_tensor, outputs=preds)  \n",
        "\n",
        "##################################\n",
        "for layer in model.layers:\n",
        "  layer.trainable = True\n",
        "model.summary()\n",
        "filepath=\"drive/MyDrive/OCT/models/%s-{epoch:02d}-{val_accuracy:.4f}.hdf5\"%name\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', save_best_only=False, mode='max',save_weights_only=True) #creating checkpoint to save the best validation accuracy\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "lr_schedule =keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.045,\n",
        "    decay_steps=2*int(train_img_num/batch_size),\n",
        "    decay_rate=0.94,\n",
        "    staircase=True)\n",
        "optimizer=keras.optimizers.SGD(momentum=0.9,learning_rate=lr_schedule)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "hist=model.fit_generator(train_generator, epochs=40,validation_data=validation_generator,shuffle=True,callbacks=callbacks_list) #start training\n",
        "with open('{}-results.csv'.format(name), mode='w',newline='') as csv_file: #write evaluation metrics\n",
        "  csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  for key in hist.history:\n",
        "    data=[key]\n",
        "    data.extend(hist.history[key])\n",
        "    csv_writer.writerow(data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}